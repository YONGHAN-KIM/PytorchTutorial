{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "- https://pytorch.org/docs/0.4.0/nn.html\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\n",
    "- https://github.com/DSKSD/Pytorch_Fast_Campus_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T06:48:26.173747Z",
     "start_time": "2018-08-22T06:48:25.734011Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a PyTorch?\n",
    "- Tensor와 Optimizer, Neural Net등 GPU연산에 최적화된 모듈을 이용하여 빠르게 딥러닝 모델을 구현할 수 있는 프레임워크\n",
    "Facebook이 밀고 있던 lua기반의 torch를 python버전으로 포팅함.\n",
    "\n",
    "# > Pytorch Basic\n",
    "- Tensor\n",
    "- autograd\n",
    "- nn\n",
    "- optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. optimizer\n",
    "### 4.1 Loss\n",
    "- 최적화를 하기 이전에 Loss를 정의해야함.\n",
    "- Loss는 output(결과값 or 예측값)과 target(실제값)가 얼마나 차이가 나는지 나타내는 측도(measure)로 목적에 맞게 loss function을 정의해야함.\n",
    "    - example1. 몸무게를 예측하는 경우 MSELoss를 계삼함\n",
    "    - examapl2. 꽃의 종류를 예측하는 경우 CrossEntorpy를 계산함\n",
    "    \n",
    "- torch.nn 패키지에 Loss를 계산해주는 다양한 loss function들이 있음.\n",
    "    - L1Loss\n",
    "    - MSELoss\n",
    "    - CrossEntorpyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 nn.MSELoss\n",
    "- input과 target에 대한 Mean Sqaured Error를 계산함.\n",
    "$$ Loss(x,y) = \\frac{1}{n}\\sum_{i}(pred_{i}-target_{i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T06:48:27.141120Z",
     "start_time": "2018-08-22T06:48:27.133427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.MSELoss() \n",
    "preds= torch.FloatTensor([[1,2],[2,0]])\n",
    "targets= torch.ones(2,2)\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 CrossEntropyLoss\n",
    "- Classification 문제에서 주로 사용되는 loss function으로 두 확률분포 사이의 차이를 재는 측도 중 하나임.\n",
    "- input과 target에 대한 CrossEntropyLoss를 계산함.\n",
    "- input은 FloatTensor type으로 클래스 수만큼 길이인 array로 구성됨.\n",
    "- target은 LongTensor로 정답인 클래스의 index로 구성됨.\n",
    "- CrossEntropy:\n",
    "$$ H(Target,Pred)= - \\sum_{x}Target(x)\\log(Q(x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T06:58:55.903390Z",
     "start_time": "2018-08-22T06:58:55.897927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9037)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.CrossEntropyLoss()\n",
    "preds= torch.FloatTensor([[0.7, 1.2, 0.6], [0.3, 1, 0.8]])\n",
    "targets= torch.LongTensor([1, 2])\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 optim\n",
    "- torch.optim 패키지를 이용해 모델의 output(결과값)과 target(실제값)에 차이인 loss를 줄이기 위해 parameter update를 통해 최적화함.  \n",
    "- 'Gradient Descent'라는 방법을 사용해 neural network의 parameter update를 함.  \n",
    "$ param : \\theta$ , $ learning rate: \\alpha$ 라고 하면 다음과 같이 업데이트 함.$$ \\theta = \\theta - \\alpha * \\frac{\\partial{loss}}{\\partial{\\theta}}$$\n",
    "- torch.optim은 다양한 optimizer를 가짐.\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adam\n",
    "    \n",
    "- 참고 사이트  \n",
    "    http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T07:15:04.024688Z",
     "start_time": "2018-08-22T07:15:04.014074Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preds] tensor([[ 1.1328],\n",
      "        [ 0.4801],\n",
      "        [ 0.7273],\n",
      "        [ 1.4749],\n",
      "        [ 1.0899]])\n",
      "[loss] tensor(0.9327)\n"
     ]
    }
   ],
   "source": [
    "inputs= torch.randn(5,3)\n",
    "targets= torch.randn(5).unsqueeze(1)\n",
    "\n",
    "# model, loss function, optimizer 인스턴스를 생성\n",
    "model= nn.Linear(3,1)\n",
    "loss_function= nn.MSELoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "preds= model(inputs)\n",
    "print('[preds]', preds)\n",
    "loss=loss_function(preds, targets)\n",
    "print('[loss]',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T07:15:37.206906Z",
     "start_time": "2018-08-22T07:15:37.198018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward 전\n",
      "None\n",
      "None\n",
      "backward 후\n",
      "tensor([[ 0.7564, -0.2288, -1.1214]])\n",
      "tensor([ 0.5350])\n"
     ]
    }
   ],
   "source": [
    "print('backward 전')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "# 각 parameter에 대한 loss에 미분값을 계산\n",
    "loss.backward()\n",
    "\n",
    "print('backward 후')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전과 후를 비교해보면 각 parameter - grad * learning rate로 업데이트 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T07:16:45.629182Z",
     "start_time": "2018-08-22T07:16:45.619108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer 전\n",
      "tensor([[ 0.0708,  0.6420,  0.3455]])\n",
      "tensor([ 0.2439])\n",
      "optimizer 전\n",
      "tensor([[-0.0048,  0.6648,  0.4576]])\n",
      "tensor([ 0.1904])\n"
     ]
    }
   ],
   "source": [
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)\n",
    "\n",
    "# optimizer로 최적화함\n",
    "optimizer.step()\n",
    "\n",
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T07:18:20.820112Z",
     "start_time": "2018-08-22T07:18:20.795562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer 전\n",
      "tensor([[ 0.3847,  0.5270,  0.4791]])\n",
      "tensor([ 0.5525])\n",
      "optimizer 전\n",
      "tensor([[ 0.3843,  0.5252,  0.4817]])\n",
      "tensor([ 0.5532])\n",
      "[Loss] 0.4111393094062805\n",
      "optimizer 전\n",
      "tensor([[ 0.3843,  0.5252,  0.4817]])\n",
      "tensor([ 0.5532])\n",
      "optimizer 전\n",
      "tensor([[ 0.3837,  0.5234,  0.4840]])\n",
      "tensor([ 0.5540])\n",
      "[Loss] 0.4110349714756012\n",
      "optimizer 전\n",
      "tensor([[ 0.3837,  0.5234,  0.4840]])\n",
      "tensor([ 0.5540])\n",
      "optimizer 전\n",
      "tensor([[ 0.3832,  0.5216,  0.4861]])\n",
      "tensor([ 0.5546])\n",
      "[Loss] 0.4109451174736023\n",
      "optimizer 전\n",
      "tensor([[ 0.3832,  0.5216,  0.4861]])\n",
      "tensor([ 0.5546])\n",
      "optimizer 전\n",
      "tensor([[ 0.3826,  0.5199,  0.4879]])\n",
      "tensor([ 0.5553])\n",
      "[Loss] 0.4108666479587555\n",
      "optimizer 전\n",
      "tensor([[ 0.3826,  0.5199,  0.4879]])\n",
      "tensor([ 0.5553])\n",
      "optimizer 전\n",
      "tensor([[ 0.3820,  0.5181,  0.4895]])\n",
      "tensor([ 0.5559])\n",
      "[Loss] 0.410797119140625\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model.zero_grad()\n",
    "    preds= model(inputs)\n",
    "    loss= loss_function(preds, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    print('optimizer 전')\n",
    "    for p in model.parameters():\n",
    "        print(p.data)\n",
    "\n",
    "    # optimizer로 최적화함\n",
    "    optimizer.step()\n",
    "\n",
    "    print('optimizer 전')\n",
    "    for p in model.parameters():\n",
    "        print(p.data)\n",
    "    print('[Loss]',loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
