{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "- https://pytorch.org/docs/0.4.0/nn.html\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\n",
    "- https://github.com/DSKSD/Pytorch_Fast_Campus_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:36:21.360667Z",
     "start_time": "2018-08-22T05:36:20.950066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a PyTorch?\n",
    "- Tensor와 Optimizer, Neural Net등 GPU연산에 최적화된 모듈을 이용하여 빠르게 딥러닝 모델을 구현할 수 있는 프레임워크\n",
    "Facebook이 밀고 있던 lua기반의 torch를 python버전으로 포팅함.\n",
    "\n",
    "# > Pytorch Basic\n",
    "- Tensor\n",
    "- autograd\n",
    "- nn\n",
    "- optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. autograd\n",
    "- 딥러닝 프레임워크의 가장 큰 장점은 자동으로 미분을 계산해주는 것임.\n",
    "- 딥러닝에서 forward 단계에서 autograd은 수행하는 모든 연산을 기억함. 그리고 역전파 단계에서  미분을 계산한.\n",
    "\n",
    "### 2.1 Tensor\n",
    "- autograd에서 requires_grad=True로 설정되면 Tensor의 연산은 기록됨. 역전파(backward)연산 후에는 이 변수에 대한 gradient(변화도)는 .grad에 누적됨.\n",
    "- autorgrad구현에서 Function클래스도 매우 중요한데 Tensor와 Function은 상호 연결되어 있으며 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성함. 각 변수는 .grad_fn속성을 갖는데 이는 생성한 Function을 참조하고 있음.(단, 사용자가 만든 Tensor는 예외로, 이 때 grad_fn은 None)임.\n",
    "- gradient를 계산하기 위해서는, Tensor의 .backward()를 호출하면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:36:25.611276Z",
     "start_time": "2018-08-22T05:36:25.604504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor를 생성하고 연산을 추적하기 위해 requries_grad=True로 설정함.\n",
    "x= torch.ones(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:36:33.983933Z",
     "start_time": "2018-08-22T05:36:33.980354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:36:37.039029Z",
     "start_time": "2018-08-22T05:36:37.036210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) # 현재 gradient를 계산하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:36:44.247290Z",
     "start_time": "2018-08-22T05:36:44.244565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn) # 생성된 텐서임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:37:16.424470Z",
     "start_time": "2018-08-22T05:37:16.420504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "# x에 대해 연산을 수행.\n",
    "y= x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:37:21.583368Z",
     "start_time": "2018-08-22T05:37:21.579725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x10f7cdef0>\n"
     ]
    }
   ],
   "source": [
    "# 연산의 결과로 생성된 것이므로 grad_fn을 가짐.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:49:27.833068Z",
     "start_time": "2018-08-22T05:49:27.828122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3.6049,   8.3441],\n",
      "        [  3.2794,  22.9086]])\n",
      "tensor(9.5343)\n"
     ]
    }
   ],
   "source": [
    "# y에 다른 연산을 수행\n",
    "z= y*y*3\n",
    "out= z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:49:29.527056Z",
     "start_time": "2018-08-22T05:49:29.523452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x1147a5a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 변화도\n",
    "- backwar에서 Tensor가 Scalar인 경우는 backward에 인자를 정해줄 필요가 없음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:52:22.819074Z",
     "start_time": "2018-08-21T06:52:22.813857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.)\n",
      "tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$out= \\frac{1}{4}\\sum_{i}z_{i}$$  \n",
    "$$z_{i}= 3(x_{i}+2)^{2}$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{z_{i}}}=\\frac{z_{i}}{4}$$  \n",
    "$$\\frac{\\partial{z_{i}}}{\\partial{x_{i}}}=6(x_{i}+2)$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{x_{i}}}=\\frac{\\partial{out}}{\\partial{z_{i}}} * \\frac{\\partial{z_{i}}}{\\partial{x_{i}}} = \\frac{z_{i}}{4}*6(x_{i}+2) =\\frac{3}{2}(x_{i}+2)$$  \n",
    "$$\\frac{\\partial{out}}{\\partial{x_{i}}}\\mid_{x_{i}=1}= \\frac{9}{2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 여러개의 요소를 가질 때는 Tensor의 모양을 gradient의 인자로 지정해줄 필요가 있음.\n",
    "- 인자로 들어간 Tensor가 값이 1일 때는 순수한 gradient이고 값이 k일 때는 gradient*k가 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-21T06:55:12.716425Z",
     "start_time": "2018-08-21T06:55:12.711866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:44:19.643447Z",
     "start_time": "2018-08-22T05:44:19.639972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "print(y)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:44:55.517787Z",
     "start_time": "2018-08-22T05:44:55.512431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.],\n",
       "        [ 1.,  1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:44:43.213274Z",
     "start_time": "2018-08-22T05:44:43.207303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  2.],\n",
      "        [ 2.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward(torch.ones(2,2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:45:12.333890Z",
     "start_time": "2018-08-22T05:45:12.329674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:45:40.934122Z",
     "start_time": "2018-08-22T05:45:40.897002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  6.],\n",
      "        [ 6.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.ones(2,2, requires_grad=True)\n",
    "y= 2*x+1\n",
    "try:\n",
    "    y.backward(torch.ones(2,2)*3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T09:28:55.619524Z",
     "start_time": "2018-08-11T09:28:55.615555Z"
    }
   },
   "source": [
    "- 기본적으로 변화도 연산은 그래프 상의 모든 내부 버퍼를 새로쓰기(flush) 떄문에, 그래프의 특정 부분에 대해서 역전판 연산을 2번하고 싶다면, 첫 연산 단계에서 retain_variables=True의 값을 넘겨줘야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:46:07.759970Z",
     "start_time": "2018-08-22T05:46:07.749371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 13.2908,  10.1354],\n",
      "        [  8.4934,  12.9705]])\n",
      "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(2,2, requires_grad=True)\n",
    "y= 2*x+2\n",
    "\n",
    "z= y*y+2\n",
    "z= z.sum()\n",
    "try:\n",
    "    z.backward() # retain_graph=True로 하면 내부 버퍼가 사라지는 것을 막아줌.\n",
    "\n",
    "    print(x.grad)\n",
    "\n",
    "    z.backward()\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:46:58.721232Z",
     "start_time": "2018-08-22T05:46:58.710343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.3848,   6.6710],\n",
      "        [ -4.1821,  11.0535]])\n",
      "tensor([[  8.7695,  13.3419],\n",
      "        [ -8.3643,  22.1069]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(2,2, requires_grad=True)\n",
    "y= 2*x+2\n",
    "\n",
    "z= y*y+2\n",
    "z= z.sum()\n",
    "try:\n",
    "    z.backward(retain_graph=True) # retain_graph=True로 하면 내부 버퍼가 사라지는 것을 막아줌.\n",
    "\n",
    "    print(x.grad)\n",
    "\n",
    "    z.backward()\n",
    "    print(x.grad)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with torch.no_grad():로 코드 블럭을 사용하면 autograd가 requires_graph=True인 Tensor들의 연산 기록을 추적을 제외할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-22T05:47:15.353984Z",
     "start_time": "2018-08-22T05:47:15.349539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
